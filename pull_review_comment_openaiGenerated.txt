{"post_id": 1158529069, "text": "Consider naming: chatgpt-shell-dall-e-image-size", "html": "https://github.com/rfaulkner7/2-FA/pull/13", "labels": ["[GPT Integration]"]}
{"post_id": 1146013779, "text": "Can we add a link to `GPT-4`?", "html": "https://github.com/reviewpad/docs/pull/254#discussion_r1146013779", "labels": ["[Unrelated]"]}
{"post_id": 1138954001, "text": "Me too! Thanks Chat GPT :D ", "html": "https://github.com/contentful/template-blog-webapp-nextjs/pull/62#discussion_r1138954001", "labels": ["[Unrelated]"]}
{"post_id": 1152342717, "text": "@openai Can you please review this block of code?", "html": "https://github.com/abidiqbalxgrid/my_repo/pull/3#discussion_r1152342717", "labels": ["[Code Review]"]}
{"post_id": 1144122871, "text": "So we'll use the endpoint powered by ChatGPT as the only one till we have the front-end to support both?", "html": "https://github.com/techstartucalgary/lifeline/pull/254#discussion_r1144122871", "labels": ["[GPT Integration]"]}
{"post_id": 1080758541, "text": "yeah we also faced some issue with the code 001 model, something possibly changed on OpenAI side. ", "html": "https://github.com/microsoft/semantic_parsing_with_constrained_lm/pull/12#discussion_r1080758541", "labels": ["[Unrelated]"]}
{"post_id": 1116263463, "text": "or use chatGPT or Bing chat to resolve it", "html": "https://github.com/aristoteleo/dynamo-release/pull/406#discussion_r1116263463", "labels": ["[Unrelated]"]}
{"post_id": 1052460859, "text": "Recommending removing `copilot` given it's a subscription service. ", "html": "https://github.com/BayAreaMetro/tm2py/pull/64#discussion_r1052460859", "labels": ["[Unrelated]"]}
{"post_id": 1130620054, "text": ":robot: ChatGPT: This patch appears to have no changes in addition to the previous ones.", "html": "https://github.com/fluxninja/aperture/pull/1544#discussion_r1130620054", "labels": ["[Unrelated]"]}
{"post_id": 1105849830, "text": "/cr-gpt", "html": "https://github.com/zqx123321/OucFly/pull/3#discussion_r1105849830", "labels": ["[Unrelated]"]}
{"post_id": 1178750747, "text": "zmien nazwe komendy na openai.chatgpt", "html": "https://github.com/G1ANT-Robot/G1ANT.Addon.OpenAI/pull/1#discussion_r1178750747", "labels": ["[GPT Integration]"]}
{"post_id": 1176817266, "text": "bravo cosmin (hai chat gpt)!!!!", "html": "https://github.com/TheG-O-A-T-S/CO-Project/pull/21#discussion_r1176817266", "labels": ["[Unrelated]"]}
{"post_id": 1170175691, "text": "I believe we can verify without `openai`. with axios and catch.", "html": "https://github.com/Royal-lobster/ChatDockX/pull/9#discussion_r1170175691", "labels": ["[Unrelated]"]}
{"post_id": 1142694507, "text": "@openai can you summarize the changes made to this file?", "html": "https://github.com/fluxninja/openai-pr-reviewer/pull/69#discussion_r1142694507", "labels": ["[Code summarization]"]}
{"post_id": 1166633349, "text": "The name presumes that this class can be used for any client as a base one. However, it's not the case, because it relies on OpenAIClient and all its functionality is about AI domain. Please consider renaming it to AIClientBase to convey AI affiliation clearly.", "html": "https://github.com/microsoft/semantic-kernel/pull/420#discussion_r1166633349", "labels": ["[Unrelated]"]}
{"post_id": 1173359495, "text": "Deze code gaf ChatGPT inderdaad ook terug, maar deze klopt niet. volgens mij. De code kan in ieder geval niet in de seeders, omdat hij $factory niet kent. Weet jij hoe je dit moet implementeren?", "html": "https://github.com/Lamineslot/projectagile-SOi/pull/43#discussion_r1173359495", "labels": ["[GPT Integration]"]}
{"post_id": 1142449338, "text": "We can cover this with tests if we make `errorAccumulator` an interface and provide failing one like here https://github.com/sashabaranov/go-openai/blob/aa149c1bf8ccd77403e07fdb7b0122f27d4c9b6b/request_builder_test.go#L30", "html": "https://github.com/sashabaranov/go-openai/pull/184#discussion_r1142449338", "labels": ["[Code Review]"]}
{"post_id": 1059495157, "text": "This is not set in GPT configs, I think it's only for encoder/decoder architectures. So this will crash if not set. Please use .get() so that it returns None if not found.", "html": "https://github.com/NVIDIA/NeMo/pull/5672#discussion_r1059495157", "labels": ["[GPT Integration]"]}
{"post_id": 1145267762, "text": "It's been ages since I worked on this code unfortunately...could the extra `None`s result in mismatched number of communications in the backward pass from stage? I think that was the original intention. Our first version of pipelined GPT in Megatron used floating-point attention masks, but once switching to non-differentiable types things got trickier.", "html": "https://github.com/Melon-Tropics/bitcoin/pull/1844", "labels": ["[Unrelated]"]}
{"post_id": 1140609835, "text": "I think it's possible to have a single implementation for all floating point types using `dtype.primitive_bitwidth` https://github.com/openai/triton/blob/128ada30439d802ab7e3cfeb5c2db43030a688e9/python/triton/language/core.py#L64. Also, is `abs` currently tested in `test_core.py`? If not, maybe we should add a test for it", "html": "https://github.com/openai/triton/pull/1361#discussion_r1140609835", "labels": ["[Unrelated]"]}
{"post_id": 1159621153, "text": "Hey @agnieszka-m I don't think is correct. `max_length` refers to the maximum number of tokens that can be generated in the output. Unfortunately, there isn't a definitive way of calculating the length of a token in characters (and it changes depending on what model you use). Here is an [article](https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them) from OpenAI that explains this concept. ", "html": "https://github.com/deepset-ai/haystack/pull/4601#discussion_r1159621153", "labels": ["[GPT Integration]"]}
{"post_id": 1123400221, "text": "could you check the grammar of the content? especially the last statement about quarter. I am not that versed but can use ChatGPT.", "html": "https://github.com/xonsh/xonsh/pull/5078#discussion_r1123400221", "labels": ["[GPT Integration]"]}
{"post_id": 1144113541, "text": "ideally we could load all registries the same way, but looks like model graded specs are special in that there's only one entry per file. is it too late to change that? cc @andrew-openai ", "html": "https://github.com/openai/evals/pull/392#discussion_r1144113541", "labels": ["[Unrelated]"]}
{"post_id": 1176507963, "text": "this is tricky because Azure does NOT support \"gpt-3.5-turbo\" as a deployment name, although it is the name of the model. This is the reason it is hardcoded. ", "html": "https://github.com/feedbackfruits/ruby-openai/pull/1#discussion_r1176507963", "labels": ["[GPT Integration]"]}
{"post_id": 1150638600, "text": "GPT-2. ", "html": "https://github.com/huseyinsimsekk/Asp.NetCore.MVC.SbAdmin2/pull/18", "labels": ["[Unrelated]"]}
